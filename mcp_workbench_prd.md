# ğŸ§ª MCP Workbench â€” Product Requirements Document (PRD)

**Project Name:** `mcp-workbench`  
**Owner:** OwlAI Engineering  
**Version:** 1.0  
**Status:** Draft  
**Last Updated:** 2025-03-28

---

## ğŸ§­ Purpose

The goal of `mcp-workbench` is to provide an automated, version-aware compliance test suite for validating whether a given **MCP (Model Context Protocol)** server implementation conforms to the official specification (e.g., `2024-11-05`). This ensures interoperability, stability, and protocol correctness across server implementations.

---

## ğŸ¯ Goals & Non-Goals

### Goals

- âœ… Validate MCP server behavior against a specific spec version
- âœ… Support all server-facing features (Prompts, Resources, Tools, Utilities)
- âœ… Detect violations of MUST/SHOULD requirements per feature
- âœ… Support version-specific test logic
- âœ… Provide clear CLI interface and human-readable test reports
- âœ… Enable integration into CI pipelines

### Non-Goals

- âŒ Testing client or host behavior (will be handled in future phases)
- âŒ Automated fuzzing or security testing
- âŒ Backend implementation or protocol enforcement (read-only test client)

---

## ğŸ‘¤ Target Users

- OwlAI engineers and QA testers
- Protocol implementers and server developers
- Integration engineers verifying conformance

---

## ğŸ’¼ Use Cases

1. **Protocol Engineer** runs `mcp-workbench` against a dev server to check compliance.
2. **CI Pipeline** runs the tool after deploying a new MCP version.
3. **External Developer** validates their MCP-compatible server implementation before release.

---

## ğŸ§± Key Features & Requirements

| ID    | Feature                       | Description                                                                 | Priority | Type        |
|-------|-------------------------------|-----------------------------------------------------------------------------|----------|-------------|
| F1.1  | Version-aware test runner      | CLI supports `--spec-version`; tests run against correct assertions         | High     | Functional  |
| F1.2  | prompts/list test              | Verifies list of prompts returned with required structure                   | High     | Functional  |
| F1.3  | prompts/get test               | Verifies prompt content, roles, content types, and argument handling        | High     | Functional  |
| F1.4  | prompts/list_changed test      | Optionally detects notifications if `listChanged` is declared               | Medium   | Functional  |
| F2.1  | resources/list + read tests    | Validates resource listing and content reading                              | High     | Functional  |
| F2.2  | resources/list_changed         | Optional support for resource change notifications                          | Medium   | Functional  |
| F2.3  | resources/subscribe test       | Validates subscriptions and updated notifications                           | Medium   | Functional  |
| F3.1  | tools/list and tools/call test | Validates tool discovery, schema, result structure, error handling          | High     | Functional  |
| F4.1  | completion/complete test       | Validates completion behavior for prompts/resources                         | Medium   | Functional  |
| F5.1  | Error response handling        | Verifies standard JSON-RPC error codes for invalid input                    | High     | Functional  |
| F6.1  | Report output                  | Test runner shows pass/fail + reason per assertion (CLI + optional JSON)    | High     | UX          |
| F6.2  | CI-compatible return codes     | Exit code = 1 on failure of any MUST requirement                             | High     | UX          |

---

## âš™ï¸ Technical Requirements

- Python 3.9+
- Pure client-side JSON-RPC (via HTTP POST)
- Configurable server URL (`--server-url`)
- Configurable spec version (`--spec-version`)
- Modular test structure (one test file per feature group)
- Uses `pytest`, `httpx`, `pydantic`

---

## ğŸ“ Architecture Overview

```
mcp-workbench/
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_prompts_*.py
â”‚   â”œâ”€â”€ test_resources_*.py
â”‚   â”œâ”€â”€ test_tools_*.py
â”‚   â””â”€â”€ test_utilities_*.py
â”œâ”€â”€ mcp/
â”‚   â”œâ”€â”€ client.py         # JSON-RPC client
â”‚   â”œâ”€â”€ protocol.py       # Data models
â”‚   â”œâ”€â”€ spec_registry.py  # Supported versions & metadata
â”œâ”€â”€ conftest.py           # Shared fixtures
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ pytest.ini
â””â”€â”€ cli.py                # Optional CLI wrapper
```

---

## ğŸ§ª Testing & QA Strategy

- Use `pytest` as the test runner
- Fixtures share server and version configuration
- Each test is tagged by feature and requirement level (MUST/SHOULD)
- Compliance report printed to stdout + optional JSON

---

## ğŸš€ Milestones

| Sprint | Milestone                              | Features                                |
|--------|-----------------------------------------|------------------------------------------|
| 1      | Prompts compliance MVP                  | F1.1 â†’ F1.4, F5.1, F6.1, F6.2             |
| 2      | Resources test support                  | F2.1 â†’ F2.3                               |
| 3      | Tools and completion testing            | F3.1, F4.1                                |
| 4      | Full version switching + CI harness     | CLI polish, export, advanced reporting   |

---

## ğŸ“ Open Questions

- Should version-specific requirements be stored as code, JSON, or markdown?
- Do we want machine-readable compliance output (e.g. JUnit XML or JSON)?
- Should we allow test skipping if a capability is not declared (e.g., listChanged)?